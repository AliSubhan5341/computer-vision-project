
# Computerâ€‘Visionâ€‘Project  
*Endâ€‘toâ€‘End Chinese Licenceâ€‘Plate DetectionÂ &Â Recognition on CCPD*

**Author:** AliÂ Subhan  
**Repo:** <https://github.com/AliSubhan5341/computer-vision-project>  
**Last update:** 18Â JunÂ 2025

---

## Table of Contents
1. [Project Overview](#1-project-overview)  
2. [Dataset: CCPD Explained](#2-dataset-ccpd-explained)  
3. [Repository Layout](#3-repository-layout)  
4. [Installation](#4-installation)  
5. [HowÂ toÂ Run](#5-how-to-run)  
6. [Implementation Details](#6-implementation-details)  
7. [Evaluation Protocol](#7-evaluation-protocol)  
8. [ResultsÂ &Â Speed Profile](#8-results--speed-profile)  
9. [LimitationsÂ &Â Future Work](#9-limitations--future-work)  
10. [References](#10-references)  
11. [License](#11-license)  

---

## 1Â Â Project Overview
This repository contains **two independent licenceâ€‘plate recognition systems**
implemented on the Chinese City Parking Dataset (**CCPD**).  
The aim is to demonstrate how modern detection and sequenceâ€‘modelling
techniques improve accuracy and robustness over the original 2018 baseline,
while also quantifying the latency cost.

| Pipeline folder | Detector | Recogniser | Origin paper |
|-----------------|----------|------------|--------------|
| `Baseline/` | Lightweight SSDâ€‘style CNN | **RPnet** (ROIâ€‘pool + 7 softmax heads) | XuÂ *etâ€¯al.*, *ECCVÂ 2018* |
| `YOLO+PDLPR/` | **YOLOv5â€‘s** (Ultralytics) | **PDLPR** (IGFE + Transformer encoder/parallel decoder) | TaoÂ *etâ€¯al.*, *SensorsÂ 2024* |

Both pipelines follow exactly the same data interface so their results can be
compared under identical conditions.

> **Note on borrowed code and baseline re-implementation**  
> * Dataset helpers (`provinces`, `alphabets`, `ads` lookup tables,  
>   `filename_to_indices()` parser, IoU-tilt utilities, etc.) are **ported directly**  
>   from the original CCPD GitHub repository  
>   <https://github.com/detectRecog/CCPD>.  Only Python-3 / PyTorch-1.12 syntax
>   was modernisedâ€”algorithmic logic is unchangedâ€”so label handling behaves
>   exactly as in the 2018 release.
>
> * The **Baseline** folder contains a **from-scratch, up-to-date re-implementation
>   of RPnet**, the reference network proposed in that same CCPD repo.  
>   Obsolete THNN ROI-pool layers were replaced by `torchvision.ops.roi_align`
>   and the training loop was rewritten to use current PyTorch APIs, but the
>   architecture, loss functions, and evaluation criteria are faithful to the
>   original paper.

---

## 2Â Â Dataset: CCPD Explained
The Chinese City Parking Dataset (CCPD) is, to date, the largest
publicly-available collection of authentic licence-plate photographs taken in
real urban traffic.
It was released by the University of Science & Technology of China (USTC) in
2018 to encourage end-to-end plate-reading research.
The **Chinese City Parking Dataset (CCPD)** is the deâ€‘facto benchmark for
Chinese licenceâ€‘plate research.

![](images/1.png)


| Aspect | Detail |
|--------|--------|
| **Origin** | Images captured by parking inspectorsâ€™ POS devices in Hefei, China. |
| **Resolution** | 720â€¯Ã—â€¯1160 RGB â€“ includes car & street context. |
| **Size** | â‰ˆâ€¯250â€¯k in *Base* split + â‰ˆâ€¯40â€¯k â€œhardâ€ splits (`Rotate`, `Tilt`, `Challenge`, â€¦). |
| **Plate pattern** | æ±‰å­— (province) + Latin letter + 5 alphanumerics. |
| **Annotation** | Embedded in the fileâ€‘name:<br>â€¢ plate area % & tilt<br>â€¢ 4â€‘vertex box + axisâ€‘aligned bbox<br>â€¢ blur & brightness<br>â€¢ **7 indices â†’ 7 glyphs** |
| **Licence** | CCÂ BYâ€‘NCÂ 4.0 (researchâ€‘only). |

**Why CCPD is special**  
Unlike datasets that ship XML / JSON, CCPD bakes *all* metadata into the
file-name.  This removes the need for parsing label files at runtime and makes
dataset handling extremely lightweight.

> **Helper functions**  
> The lookup tables (`provinces`, `alphabets`, `ads`) and the filename parser
> were copied from the original CCPD repo and upgraded to PythonÂ 3 /
> PyTorchÂ 1.12.  Logic is unchanged and is used by *both* pipelines.

### Filename anatomy  

```
025-95_113-154&383_386&473-386&473_177&454_154&383_363&402-0_0_22_27_27_33_16-37-15.jpg
â”‚ â”‚ â”‚            bbox coords            â”‚           â”‚              â”‚    â”‚  â”‚
â”‚ â”‚ â”‚                                   vertices     7 indices      br   blur
â”‚ â”‚ â”” tilt h_v Ã—0.1Â° 
â”‚ â”” area %
â”” image timestamp
```
---

## 3Â Â Repository Layout
Each subâ€‘project is organised into the **Globals â†’ Utils â†’ Data â†’ Network â†’
Train â†’ Evaluation** hierarchy, making it easy to swap components.

```
computer-vision-project/
â”œâ”€â”€ Baseline/                # ECCVâ€‘18 RPnet
â”‚   â”œâ”€â”€ globals.py  â† constants, lookup tables
â”‚   â”œâ”€â”€ utils.py    â† helper functions
â”‚   â”œâ”€â”€ data.py     â† CCPDPlateCrops Dataset
â”‚   â”œâ”€â”€ network.py  â† RPnet backbone + heads
â”‚   â”œâ”€â”€ train.py    â† training loop
â”‚   â””â”€â”€ evaluation.py
â”‚
â”œâ”€â”€ YOLO+PDLPR/
â”‚   â”œâ”€â”€ YOLO/      â† detector wrapper & training scripts
â”‚   â””â”€â”€ PDLPR/     â† recogniser code in same 6â€‘file pattern
â”‚
â”œâ”€â”€ checkpoints/   â† saved *.pth weights
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

*Having identical module boundaries means you can, for example, plug the
PDLPR recogniser into the Baseline detector with minimal glue.*

---

## 4Â Â Installation
A stepâ€‘byâ€‘step guide that installs **only** the packages actually required by
the folders in the screenshots.

```bash
# clone repo
git clone https://github.com/AliSubhan5341/computer-vision-project.git
cd computer-vision-project

# create isolated env
conda create -n ccpd python=3.10 -y
conda activate ccpd

# 1ï¸âƒ£  libraries used by recognisers and baseline
pip install -r YOLO+PDLPR/PDLPR/requirements.txt  # torch, torchvision, Pillowâ€¦

# 2ï¸âƒ£  Ultralytics for YOLO detector
pip install ultralytics==8.*

# 3ï¸âƒ£  (optional) deployâ€‘time speedâ€‘ups
pip install onnxruntime-gpu tensorrt
```

> **Tip:** if you only want the Baseline pipeline you can skip Ultralytics.

---

## 5Â Â HowÂ toÂ Run
### Prepare data
```
data/
  raw_images/           â† unzip ccpd_base images here
```
*(Hard splits can be added as sibling folders; loaders ignore them unless asked.)*

### 5.1Â Baseline PipelineÂ (RPnetÂ 2018)
```bash
cd Baseline
python train.py       --data_root ../data         # train 30Â epochs
python evaluation.py  --data_root ../data         # OA + FPS
```

### 5.2Â Modern PipelineÂ (YOLOv5Â +Â PDLPRÂ 2024)
```bash
# A. YOLO detector training
cd YOLO+PDLPR/YOLO
python train.py                       # thin wrapper around ultralytics.yolo

# B. Crop plate regions
python data.py --weights runs/train/exp/weights/best.pt                --src_raw ../../data/raw_images                         --dst_crops ../../data/crops

# C. PDLPR recogniser training
cd ../PDLPR
python train.py       --data_root ../../data/crops

# D. Endâ€‘toâ€‘end evaluation
python evaluation.py  --data_root ../../data/crops                       --weights ../../checkpoints/best_*.pth
```

---

## 6Â Â Implementation Details

### 6.1Â PaperÂ 1 â€” RPnet (*ECCVÂ 2018*)
* **Detector:** shallow SSD predicts one bounding box per image.  
* **Recogniser:** ROIâ€‘pools 3 feature maps, concatenates, feeds **7 softmax
  heads**.  
* **Loss:** Smoothâ€‘L1 (bbox) + Crossâ€‘Entropy (7 glyphs).  
* **Reâ€‘implementation notes:** replaced deprecated THNN ROIâ€‘pool with
  `torchvision.ops.roi_align`, removed `Variable`.

### 6.2Â PaperÂ 2 â€” YOLOv5â€‘PDLPR (*SensorsÂ 2024*)
* **YOLOv5â€‘s** (Focus + CSPDarknet + PAN) â€” 96.7â€¯% mAP@0.7 IoU.  
* **IGFE:** Focus slice â†’ ConvDownSamplingÃ—2 â†’ ResBlockÃ—4.  
* **Encoder:** 3 blocks with 8â€‘head MHA, residual bottleneck.  
* **Parallel decoder:** 3 blocks, masked selfâ€‘MHA + crossâ€‘MHA + FFN â€”
  predicts all 7 glyphs in a single forward pass (no RNN).  
* **Loss:** CTC (only final string labelled).  
* **Speed:** ~160Â FPS on plate crops.  

Our `YOLO+PDLPR/` folder mirrors this architecture exactly; Ultralytics handles
the detector, pure PyTorch the recogniser.

---

## 7Â Â Evaluation Protocol
* **Detection success:** IoU(pred, GT)Â >Â 0.70  
* **Recognition success:** IoUÂ >Â 0.60 **and** 7â€‘glyph exact match  
* **Overall Accuracy (OA):** proportion of images satisfying both.  
* **Latency:** average wallâ€‘clock over 100 crops, including CUDA synchronisation
  for accurate GPU timing.

---

## 8Â Â ResultsÂ &Â Speed Profile
*(Replace the placeholders with your final numbers.)*

| Pipeline | Base OA | Challenge OA | Detector ms | Recogniser ms | Endâ€‘toâ€‘end FPS |
|----------|--------:|-------------:|------------:|--------------:|---------------:|
| Baseline (RPnet) | 98.0Â % | 88.9Â % | â€“   | **20.8** | **48** |
| YOLOv5Â +Â PDLPR   | **99.2Â %** | **94.0Â %** | 121 | 25 | 6.8 |

*Detector latency dominates; converting YOLO to TensorRTâ€‘FP16 can push total to
~25Â FPS.*

---

## 9Â Â LimitationsÂ &Â Future Work
* Assumes **one plate per image** â€” multiâ€‘plate scenes not handled.  
* Detector bottleneck â€” explore YOLOv8â€‘Nano or pruning for realâ€‘time.  
* No perspective correction; integrating TPS or STN could improve extreme
  tilt cases.

---

## 10Â  References
1. **XuÂ Z.Â etÂ al.** â€œTowards Endâ€‘toâ€‘End Licenseâ€‘Plate DetectionÂ â€¦â€, *ECCVÂ 2018*.  
2. **TaoÂ L.Â etÂ al.** â€œA Realâ€‘Time License Plate DetectionÂ â€¦â€, *SensorsÂ 2024*.  
3. **PrajapatiÂ R.Â etÂ al.** â€œANPR Surveyâ€, *ICCMARÂ 2023*.

---

## 11Â  License
*Source code* Â©Â 2025 AliÂ Subhan â€” MIT.  
*Dataset* Â©Â USTC & Xingtai â€” CCÂ BYâ€‘NC (researchâ€‘only).
---

*Happy training & safe drivingÂ ğŸš—ğŸ’¨*
